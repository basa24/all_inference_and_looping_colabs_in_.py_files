# -*- coding: utf-8 -*-
"""YOLOv8_LOOP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JnBRZRVe5XlYt27i_7nkP6vX-pPNfE2K
"""

# -*- coding: utf-8 -*-
"""yolov8_inference_evaluation

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wzy9OBgmEoKBuHSKj8fHfS3VOPZ_e81S
"""

!pip install torch torchvision
!pip install ultralytics -q
!pip install pyyaml -q
import torch
import pandas as pd
import time
from PIL import Image
import cv2
import os
import json
from ultralytics import YOLO
import yaml
from google.colab.patches import cv2_imshow


file_name = "../usr/local/lib/python3.10/dist-packages/ultralytics/cfg/datasets/coco8.yaml"
with open (file_name, "r") as stream:
    names = yaml.safe_load(stream)["names"]

coco_names= list(names.values())

# Load the pre-trained Faster R-CNN model
model = YOLO("yolov8n.pt")


if torch.cuda.is_available():
    print("GPU is available!")
else:
    print("GPU is not available.")
model.eval()

# Replace 'your_file.json' with the path to your actual JSON file
file_path = '/content/drive/MyDrive/files and others for ee/imagesvalidationids.json'

with open(file_path, 'r') as file:
    data = json.load(file)
file_name_to_id = {item['file_name']: item['id'] for item in data}

print(file_name_to_id)

file_path = '/content/drive/MyDrive/files and others for ee/categoriesvalidation.json'
with open(file_path, 'r') as file:
    data = json.load(file)
categoryid_to_name = {item['id']: item['name'] for item in data}

print(categoryid_to_name)

resolutions=[640, 540, 440, 340, 240, 140]

for r in resolutions:
  resolution = r
  for i in range(5):

    desired_resolution=(resolution, resolution)
    times=[]
    bboxes = []
    scores = []
    class_names = []
    image_names = []
    image_id_perdetection=[]
    from os import listdir
    directory="/content/drive/MyDrive/valid"
    for image in os.listdir(directory):
      image_path = "/content/drive/MyDrive/valid/"+image
      #print(image_path) works, every path accessed
      with Image.open(image_path) as img:
            resized_img = img.resize(desired_resolution)

      start_time = time.time()

      results = model.predict(resized_img, save_conf=True, conf =0.5)

      end_time = time.time()  # Time after model prediction

      elapsed_time = end_time - start_time
      times.append(elapsed_time)

      # Apply confidence threshold
      json_output = results[0].tojson()

    # Parse the JSON output
      detections = json.loads(json_output)



      # Extract information for each detection
      for detection in detections:
          # Extract and store bounding box coordinates
          box = detection["box"]
          bbox = [box["x1"], box["y1"], box["x2"], box["y2"]]
          bboxes.append(bbox)

          # Extract and store confidence score
          confidence = detection["confidence"]
          scores.append(confidence)

          # Extract and store class name
          class_name = detection["name"]
          class_names.append(class_name)

          image_id = file_name_to_id[image]
          image_id_perdetection.append(image_id)

    dict = {'image id': image_id_perdetection, 'name': class_names, 'bounding box': bboxes, 'confidence': scores }

    df = pd.DataFrame(dict)

    pd.set_option('display.max_rows', None)  # This will allow all rows to be displayed
    pd.set_option('display.max_columns', None)  # This will allow all columns to be displayed

    df_sorted = df.sort_values(by='image id', ascending=True)

    # If you want to modify the original DataFrame in-place, you can use:
    # df.sort_values(by='image id', ascending=True, inplace=True)

    df_sorted
    #print("\n", elapsed_time_approx,"seconds") figure this out later

    #getting data frame from anotated images
    file_path = '/content/drive/MyDrive/files and others for ee/modified_data_with_boundingboxes_try2.json'
    with open(file_path, 'r') as file:
        data = json.load(file)

    extracted_data = []
    for item in data:
        extracted_data.append({
            'image id': item['image_id'],
            'name': item['category_id'],
            'bounding box': item['bbox']
        })

    df_annot = pd.DataFrame(extracted_data)
    df_annot

    # Assuming df_sorted and df_annot are your DataFrames

    def calculate_iou(boxA, boxB):
        # Unpack the bounding box coordinates and calculate their areas
        xA, yA, wA, hA = boxA
        xB, yB, wB, hB = boxB
        areaA = wA * hA
        areaB = wB * hB

        # Convert to corner coordinates
        xAmin, yAmin, xAmax, yAmax = xA, yA, xA + wA, yA + hA
        xBmin, yBmin, xBmax, yBmax = xB, yB, xB + wB, yB + hB

        # Calculate intersection coordinates
        xImin = max(xAmin, xBmin)
        yImin = max(yAmin, yBmin)
        xImax = min(xAmax, xBmax)
        yImax = min(yAmax, yBmax)

        # Calculate intersection area
        interArea = max(xImax - xImin, 0) * max(yImax - yImin, 0)

        # Calculate IoU
        iou = interArea / float(areaA + areaB - interArea)
        return iou

    # Prepare a list to hold match results
    matches = []

    # Iterate over each prediction
    for index, row in df_sorted.iterrows():
        image_id = row['image id']
        class_name = row['name']
        pred_box = row['bounding box']

        # Filter ground truths for the same image ID and class name
        gt_filtered = df_annot[(df_annot['image id'] == image_id) & (df_annot['name'] == class_name)]

        best_iou = 0
        best_gt = None

        # Compare with each ground truth bounding box
        for _, gt_row in gt_filtered.iterrows():
            gt_box = gt_row['bounding box']
            iou = calculate_iou(pred_box, gt_box)

            if iou > best_iou:
                best_iou = iou
                best_gt = gt_box

        # If a match was found, append it to the matches list
        if best_iou > 0:  # You can set a threshold if needed
            matches.append({
                'image id': image_id,
                'class name': class_name,
                'pred box': pred_box,
                'gt box': best_gt,
                'iou': best_iou
            })

    # Convert matches to DataFrame for easier analysis
    df_matches = pd.DataFrame(matches)

    df_matches

    print("number of wrong/duplicate detections: "+ str(len(df_sorted)-len(df_matches)))
    print("number of missed detections: "+ str(len(df_annot)-len(df_matches)))

    ious = df_matches['iou']
    average_iou = sum(ious) / len(ious)
    average_time=sum(times) / len(times)
    TP = sum(iou > 0.5 for iou in ious)
    FP = len(df_sorted) - TP
    FN = len(df_annot) - TP


    precision = TP / (TP + FP)
    recall = TP / (TP + FN)
    if (precision+recall)!=0:
     f1_score = 2 * (precision * recall) / (precision + recall)
    else:
      f1_score=0
    print(f"Precision: {precision:.2f} \nRecall: {recall:.2f} \nF1 Score: {f1_score:.2f} \nAverage iou: {average_iou:.2f}\nCorrect Predictions(TP): {TP:.2f} \nMispredictions/duplicate detections(FP): {FP:.2f} \nMissed detections(FN): {FN:.2f} \nAverage time: {average_time:.2f}")

    str_resolution=str(resolution)
    data = {
        "Model": ["YOLOv8n"],
        "Image resolution": [str_resolution+" x "+str_resolution],
        "TP": [TP],
        "FP": [FP],
        "FN": [FN],
        "Precision": [precision],
        "Recall": [recall],
        "F1 Score": [f1_score],
        "Average IoU": [average_iou],
        "Average Time": [average_time]
        }

    # Create the DataFrame
    df_results = pd.DataFrame(data)
    df_results

    import numpy as np

    # Assuming df_sorted, df_annot, and df_matches are already defined as per your description

    # Filter df_matches for IoU >= 0.5
    df_matches_filtered = df_matches[df_matches['iou'] >= 0.5]

    # Define a function to calculate Average Precision (AP) for a class
    def calculate_AP(precision_recall_points):
        # Sort by recall
        precision_recall_points.sort(key=lambda x: x[0])
        # Add a point for recall = 0 with precision = 1
        precision_recall_points.insert(0, (0, 1))
        # Add a point for recall = 1 with the last precision
        precision_recall_points.append((1, precision_recall_points[-1][1]))

        # Calculate the area under the curve using the trapezoidal rule
        ap = 0
        for i in range(1, len(precision_recall_points)):
            recall_diff = precision_recall_points[i][0] - precision_recall_points[i-1][0]
            precision_avg = (precision_recall_points[i][1] + precision_recall_points[i-1][1]) / 2
            ap += recall_diff * precision_avg
        return ap

    # Get unique classes
    unique_classes = df_sorted['name'].unique()
    APs = {}

    for class_name in unique_classes:
        class_preds = df_sorted[df_sorted['name'] == class_name]
        class_gts = df_annot[df_annot['name'] == class_name].shape[0]

        TP = FP = 0
        precision_recall_points = []

        for _, pred in class_preds.iterrows():
            # Convert 'pred box' to a tuple for comparison
            pred_box_tuple = tuple(pred['bounding box'])

            # Adjusted .apply() to compare 'pred box' tuples
            match = df_matches_filtered.apply(lambda x: x['class name'] == class_name and tuple(x['pred box']) == pred_box_tuple, axis=1)

            if match.any():
                TP += 1
            else:
                FP += 1

            precision = TP / (TP + FP) if TP + FP > 0 else 0
            recall = TP / class_gts if class_gts > 0 else 0
            precision_recall_points.append((recall, precision))

        # Calculate AP for this class, using your calculate_AP function
        APs[class_name] = calculate_AP(precision_recall_points)

    # Calculate mAP by averaging APs
    mAP = np.mean(list(APs.values()))
    print(f"mAP (IoU >= 0.5): {mAP:.4f}")


    df_results["mAP"]=[mAP]
    df_results

    file_path_csv = '/content/drive/MyDrive/files and others for ee/YOLOv8n_evaluation.csv'  # Replace with your actual file path

    # Append the DataFrame to the existing CSV without including the header
    df_results.to_csv(file_path_csv, mode='a', index=False, header=False)

    print(resolution)